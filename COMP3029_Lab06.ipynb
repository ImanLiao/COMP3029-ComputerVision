{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP3029_Lab06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNww7sAr4BoWRAsQLGGEH5/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImanLiao/COMP3029-ComputerVision/blob/main/COMP3029_Lab06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqr4fyHy2Xb_"
      },
      "source": [
        "# Introduction\n",
        "This notebook is prepared by Dr. Iman Yi Liao. The main objective of the lab session is to understand and learn basic functions in torch.autograd package for building solutions to optimisation problems.\n",
        "\n",
        "Main resources:\n",
        "- https://pytorch.org/docs/stable/autograd.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sORRq5flAgb4"
      },
      "source": [
        "# Mount Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWATA1p3AjDr",
        "outputId": "e8f419a2-3899-45f8-ca34-7d7717559e9b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vBOUZHGBtxx"
      },
      "source": [
        "# 3D Face Dataset\n",
        "I had a 3D face dataset converted and stored in matlab .mat files. You can access the main folder [here](https://drive.google.com/drive/folders/1LRkrS3nnbLeX2dXgQrn_kzBP7Qz6DC6P?usp=sharing), data files [here](https://drive.google.com/drive/folders/13gi7l93wvBNvWeAfgWDbBHyoOUEZ3Yfh?usp=sharing), and labels [here](https://drive.google.com/drive/folders/1Fc-e2uH1M7G-Xw6zdH56dlDY1H3C4nDM?usp=sharing). Each 3D face has the same number of vertices. The index of each vertex for each face corresponds to that of another 3D face. There are several tasks I could do with the data.\n",
        "1. To build a model to predict the sex of a 3D face if the relevant labels are available.\n",
        "2. To build a model to cluster the 3D face dataset into two categories (hypothetically as male and female) when no labels are available\n",
        "3. To build a model to cluster a 3D face into different regions (hypothetically each region may have a correspondence to the anatomical structure of a face)\n",
        "\n",
        "You can probably think of more...but we will only demonstrate on the task 1 here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCHgCI76AybL",
        "outputId": "8c6d4b36-aa37-440f-fcae-46cdc031fe73"
      },
      "source": [
        "%cd /content/gdrive/My Drive/USF 3D Face Database"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/USF 3D Face Database\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g60UEYHF2B0"
      },
      "source": [
        "# Define the USF3DFaceDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yO7xURwF7rH"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import scipy.io as spio\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "# from torch.autograd import Variable\n",
        "import os\n",
        "import torch.optim as optim"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urVn6uklGMbv"
      },
      "source": [
        "# define custom USF 3D face dataset\n",
        "class USF3DFaceDataset(Dataset):\n",
        "    def __init__(self, data_root):\n",
        "        self.data_root = data_root\n",
        "        self.samples = []\n",
        "        self._init_dataset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return self.samples[index]\n",
        "\n",
        "    def _init_dataset(self):\n",
        "        genderfolder = os.path.join(self.data_root,'labels_gender/')\n",
        "        for gender in os.listdir(genderfolder):\n",
        "            labels = spio.loadmat(genderfolder + gender)['targets']\n",
        "            labels = torch.from_numpy(labels).float()\n",
        "\n",
        "        datafolder = os.path.join(self.data_root, 'data/')\n",
        "        for i in range(len(os.listdir(datafolder))):\n",
        "            data = spio.loadmat(datafolder + 'faceobject' + ('%s' % (i+1)))['alignedFace']\n",
        "            data = torch.from_numpy(data).float()\n",
        "            self.samples.append((data, labels[i]))\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb98cCL3HTuU",
        "outputId": "61e5f966-907b-4021-b01e-1c1c85c47cc2"
      },
      "source": [
        "# Load the dataset\n",
        "directory = '/content/gdrive/My Drive/USF 3D Face Database/'\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "                            \n",
        "facedataset = USF3DFaceDataset(directory)\n",
        "print(type(facedataset))\n",
        "print(np.shape(facedataset[0][0]), type(facedataset[0][0]))\n",
        "print('USF3D Face Dataset is loaded: %d samples' % len(facedataset))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class '__main__.USF3DFaceDataset'>\n",
            "torch.Size([75972, 3]) <class 'torch.Tensor'>\n",
            "USF3D Face Dataset is loaded: 100 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxpxuYoQTcYN"
      },
      "source": [
        "# Define my own model for sex prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEEh3jvkTfg-"
      },
      "source": [
        "# Define my own model\n",
        "class modelAutoGroupingWithGrad:\n",
        "    # model: tr{((GF)W)B}-y = 0, where F is the original data, G the grouping matrix, W the feature extractor, and B the GroupLasso coefficients, y the label\n",
        "    \n",
        "    def __init__(self,datasize,nGroups=10,dimFeatures=8):\n",
        "        self.datasize = datasize # size = [N,D] where N is the number of points and D the dimension of each point, e.g. N=1000, D=3\n",
        "        self.G = torch.rand(nGroups,self.datasize[0],requires_grad=True) # G is the grouping matrix, converting N points into K=nGroups, G is zero-one matrix\n",
        "        self.W = torch.randn(self.datasize[1],dimFeatures,requires_grad=True) # W is the feature extractor\n",
        "        self.B = torch.randn(dimFeatures,nGroups,requires_grad=True) # B is the GroupLasso coefficients\n",
        "        \n",
        "    def forward(self,x):\n",
        "        # pass the input tensor through each of the operations\n",
        "        x = torch.matmul(self.G, x)\n",
        "        x = torch.matmul(x, self.W)\n",
        "        x = torch.matmul(x, self.B)\n",
        "        x = torch.trace(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self,x,y):\n",
        "        # calculate the loss\n",
        "        # using square loss here\n",
        "        return (self.forward(x) - y)**2 \n",
        "\n",
        "    def batch_loss(self, batchdata, para_IRLS):\n",
        "        # define the average loss of a batch\n",
        "        faces, labels = batchdata\n",
        "        loss = 0\n",
        "        for i in range(len(faces)):\n",
        "            # loss += (self.forward(faces[i]) - labels[i])**2\n",
        "            loss += self.loss(faces[i], labels[i])\n",
        "\n",
        "        temp = torch.matmul(self.B, para_IRLS.sqrt())\n",
        "        temp = torch.matmul(temp.T, temp)\n",
        "        loss_L1 = torch.trace(temp) + torch.sum(1/torch.diag(para_IRLS))\n",
        "        \n",
        "        return loss/len(batchdata) + loss_L1\n",
        "        # return loss/len(batchdata)\n",
        "    \n",
        "\n",
        "    def train_with_grad(self, trainset, valset, para_IRLS, total_epoch, learning_rate):\n",
        "        # training the model with the given trainset\n",
        "        train_loader = DataLoader(trainset, batch_size=5, shuffle=True, num_workers=2)\n",
        "        val_loader = DataLoader(valset, batch_size=5, shuffle=True, num_workers=2)\n",
        "\n",
        "        # construct model parameters as iterable\n",
        "        model_parameters = iter([self.G, self.W, self.B])\n",
        "        #optimizer = optim.SGD(model_parameters, learning_rate, momentum=0.9)\n",
        "        optimizer = optim.Adam(model_parameters, learning_rate)\n",
        "\n",
        "        print(self.G)\n",
        "        \n",
        "        for epoch in range(total_epoch):\n",
        "            running_loss = 0\n",
        "            for i, batchdata in enumerate(train_loader,0):\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                loss = self.batch_loss(batchdata, para_IRLS)\n",
        "                loss.backward()\n",
        "##                self.W.data.sub_(self.W.grad * learning_rate)\n",
        "##                self.B.data.sub_(self.B.grad * learning_rate)\n",
        "##                self.G.data.sub_(self.G.grad * learning_rate)\n",
        "                optimizer.step()\n",
        "\n",
        "##                print('parameter G at batch %d in epoch %d :' % (i, epoch))\n",
        "##                print(self.G)\n",
        "                \n",
        "                running_loss += loss\n",
        "            running_loss = running_loss / i\n",
        "\n",
        "            # validating the model within the current epoch\n",
        "            with torch.no_grad():\n",
        "                val_loss = 0\n",
        "                for i, batchdata in enumerate(val_loader,0):\n",
        "                    val_loss += self.batch_loss(batchdata, para_IRLS)\n",
        "                val_loss = val_loss / i\n",
        "\n",
        "            # Here we should plot the training loss and validation loss -- to be done!!!\n",
        "            print('[Epoch %d] Training loss: %.3f, Validation loss: %.3f' % (epoch + 1,  running_loss, val_loss ))\n",
        "            \n",
        "\n",
        "    def train_IRLS(self, trainset, valset, max_IRLS_iter=50, total_epoch=50, learning_rate=0.0001):\n",
        "        ### to initialise and to define, para_IRLS is the parameter introduced by the iteratively reweighted least square method\n",
        "        \n",
        "        para_IRLS = torch.sum(torch.mul(self.B, self.B), dim=0) + 0.1e-5\n",
        "        # para_IRLS = torch.diag(torch.matmul(self.B.t(), self.B)) + 0.1e-5\n",
        "        para_IRLS = para_IRLS.detach()\n",
        "        para_IRLS = 1 / para_IRLS.sqrt()\n",
        "        para_IRLS = torch.diag(para_IRLS)\n",
        "\n",
        "        # IRLS algorithm\n",
        "        stop = False\n",
        "        n_iter = 0\n",
        "        while not stop:\n",
        "            # optimize w.r.t. G,W,B whike keeping para_IRLS fixed\n",
        "            print('IRLS iteration %d begins...' % (n_iter+1))\n",
        "            self.train_with_grad(trainset, valset, para_IRLS, total_epoch, learning_rate)\n",
        "\n",
        "            temp = torch.sum(torch.mul(self.B, self.B), dim=0) + 0.1e-5\n",
        "            temp = temp.detach()\n",
        "            temp = 1 / temp.sqrt()\n",
        "            diff = torch.norm(torch.diag(para_IRLS) - temp)\n",
        "            print('Difference between the values of IRLS parameters in two successive iterations: %.6f' % diff)\n",
        "            para_IRLS = torch.diag(temp)\n",
        "            n_iter += 1\n",
        "            stop = (diff < 1e-6) or (n_iter >= max_IRLS_iter)\n",
        "            \n",
        "\n",
        "    # def test(self, testset):\n",
        "\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        predict_labels = []\n",
        "        for i in range(len(inputs)):\n",
        "            predict_labels.append(self.forward(inputs[i]))\n",
        "\n",
        "        return predict_labels\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-jDU5FcUa6K"
      },
      "source": [
        "# Train the model with facedataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy9bJ_EgUuSg",
        "outputId": "deabd183-f3dd-48e8-d85c-904a8d3c7bef"
      },
      "source": [
        "trainset, valset = random_split(facedataset, [80, 20])\n",
        "\n",
        "# Create the auto grouping model\n",
        "size = np.shape(trainset[0][0])\n",
        "model = modelAutoGroupingWithGrad(size)\n",
        "\n",
        "# Train the model (and validate it while training)\n",
        "EPOCH = 50\n",
        "LR = 0.0001\n",
        "MAX_IRLS_ITERATION = 10\n",
        "model.train_IRLS(trainset, valset, max_IRLS_iter=MAX_IRLS_ITERATION, total_epoch=EPOCH, learning_rate=LR)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IRLS iteration 1 begins...\n",
            "tensor([[0.0671, 0.5402, 0.2105,  ..., 0.1065, 0.9351, 0.2561],\n",
            "        [0.2123, 0.2237, 0.0709,  ..., 0.0474, 0.1605, 0.1908],\n",
            "        [0.8554, 0.2697, 0.2406,  ..., 0.4311, 0.3233, 0.0775],\n",
            "        ...,\n",
            "        [0.2030, 0.4629, 0.3604,  ..., 0.8924, 0.0200, 0.3272],\n",
            "        [0.2567, 0.5851, 0.1198,  ..., 0.0579, 0.3515, 0.6080],\n",
            "        [0.4711, 0.9480, 0.7919,  ..., 0.6823, 0.3416, 0.7124]],\n",
            "       requires_grad=True)\n",
            "[Epoch 1] Training loss: 136432288.000, Validation loss: 54131468.000\n",
            "[Epoch 2] Training loss: 24762522.000, Validation loss: 50061108.000\n",
            "[Epoch 3] Training loss: 21466490.000, Validation loss: 37960708.000\n",
            "[Epoch 4] Training loss: 23203540.000, Validation loss: 38223228.000\n",
            "[Epoch 5] Training loss: 20416908.000, Validation loss: 56042880.000\n",
            "[Epoch 6] Training loss: 29289174.000, Validation loss: 49611660.000\n",
            "[Epoch 7] Training loss: 17410808.000, Validation loss: 35204508.000\n",
            "[Epoch 8] Training loss: 16110673.000, Validation loss: 32987754.000\n",
            "[Epoch 9] Training loss: 18310008.000, Validation loss: 31052310.000\n",
            "[Epoch 10] Training loss: 21853780.000, Validation loss: 28831954.000\n",
            "[Epoch 11] Training loss: 19330792.000, Validation loss: 28097778.000\n",
            "[Epoch 12] Training loss: 15285597.000, Validation loss: 48007568.000\n",
            "[Epoch 13] Training loss: 20163938.000, Validation loss: 30250914.000\n",
            "[Epoch 14] Training loss: 13148029.000, Validation loss: 30225720.000\n",
            "[Epoch 15] Training loss: 13761052.000, Validation loss: 25920342.000\n",
            "[Epoch 16] Training loss: 19327020.000, Validation loss: 26680238.000\n",
            "[Epoch 17] Training loss: 11805455.000, Validation loss: 33264736.000\n",
            "[Epoch 18] Training loss: 12555930.000, Validation loss: 31576110.000\n",
            "[Epoch 19] Training loss: 11478890.000, Validation loss: 29721958.000\n",
            "[Epoch 20] Training loss: 9815452.000, Validation loss: 24756278.000\n",
            "[Epoch 21] Training loss: 13995516.000, Validation loss: 31245142.000\n",
            "[Epoch 22] Training loss: 19790254.000, Validation loss: 25364530.000\n",
            "[Epoch 23] Training loss: 12525865.000, Validation loss: 22676232.000\n",
            "[Epoch 24] Training loss: 13331698.000, Validation loss: 25534970.000\n",
            "[Epoch 25] Training loss: 20458764.000, Validation loss: 25626802.000\n",
            "[Epoch 26] Training loss: 27641534.000, Validation loss: 52192596.000\n",
            "[Epoch 27] Training loss: 25286328.000, Validation loss: 31727210.000\n",
            "[Epoch 28] Training loss: 18578638.000, Validation loss: 33559792.000\n",
            "[Epoch 29] Training loss: 13143286.000, Validation loss: 20998070.000\n",
            "[Epoch 30] Training loss: 10365281.000, Validation loss: 19561164.000\n",
            "[Epoch 31] Training loss: 8809569.000, Validation loss: 42240928.000\n",
            "[Epoch 32] Training loss: 11481175.000, Validation loss: 18847032.000\n",
            "[Epoch 33] Training loss: 9381983.000, Validation loss: 19299358.000\n",
            "[Epoch 34] Training loss: 14201417.000, Validation loss: 38496416.000\n",
            "[Epoch 35] Training loss: 24330266.000, Validation loss: 41324292.000\n",
            "[Epoch 36] Training loss: 16766141.000, Validation loss: 22299214.000\n",
            "[Epoch 37] Training loss: 8164213.500, Validation loss: 24674544.000\n",
            "[Epoch 38] Training loss: 16026082.000, Validation loss: 33996980.000\n",
            "[Epoch 39] Training loss: 15012770.000, Validation loss: 17243142.000\n",
            "[Epoch 40] Training loss: 13779775.000, Validation loss: 20130076.000\n",
            "[Epoch 41] Training loss: 8240651.500, Validation loss: 16829270.000\n",
            "[Epoch 42] Training loss: 8093448.500, Validation loss: 21688762.000\n",
            "[Epoch 43] Training loss: 6876082.000, Validation loss: 16824030.000\n",
            "[Epoch 44] Training loss: 9363149.000, Validation loss: 16159008.000\n",
            "[Epoch 45] Training loss: 5280336.500, Validation loss: 20062942.000\n",
            "[Epoch 46] Training loss: 8803119.000, Validation loss: 15992608.000\n",
            "[Epoch 47] Training loss: 9081061.000, Validation loss: 26066928.000\n",
            "[Epoch 48] Training loss: 6700013.000, Validation loss: 18620422.000\n",
            "[Epoch 49] Training loss: 6995359.000, Validation loss: 21128256.000\n",
            "[Epoch 50] Training loss: 12362059.000, Validation loss: 17227130.000\n",
            "Difference between the values of IRLS parameters in two successive iterations: 0.002331\n",
            "IRLS iteration 2 begins...\n",
            "tensor([[0.0683, 0.5414, 0.2118,  ..., 0.1056, 0.9342, 0.2538],\n",
            "        [0.2130, 0.2244, 0.0716,  ..., 0.0387, 0.1516, 0.1806],\n",
            "        [0.8549, 0.2691, 0.2400,  ..., 0.4329, 0.3249, 0.0784],\n",
            "        ...,\n",
            "        [0.2020, 0.4620, 0.3594,  ..., 0.8967, 0.0249, 0.3371],\n",
            "        [0.2577, 0.5861, 0.1208,  ..., 0.0530, 0.3458, 0.5956],\n",
            "        [0.4717, 0.9487, 0.7925,  ..., 0.6798, 0.3393, 0.7102]],\n",
            "       requires_grad=True)\n",
            "[Epoch 1] Training loss: 65787536.000, Validation loss: 20849176.000\n",
            "[Epoch 2] Training loss: 10571497.000, Validation loss: 36280916.000\n",
            "[Epoch 3] Training loss: 9386581.000, Validation loss: 15028063.000\n",
            "[Epoch 4] Training loss: 5456240.000, Validation loss: 22355048.000\n",
            "[Epoch 5] Training loss: 9965574.000, Validation loss: 21599730.000\n",
            "[Epoch 6] Training loss: 9210227.000, Validation loss: 15701704.000\n",
            "[Epoch 7] Training loss: 6671286.500, Validation loss: 15852164.000\n",
            "[Epoch 8] Training loss: 6013723.000, Validation loss: 14576507.000\n",
            "[Epoch 9] Training loss: 4736328.500, Validation loss: 16451547.000\n",
            "[Epoch 10] Training loss: 9127370.000, Validation loss: 22977944.000\n",
            "[Epoch 11] Training loss: 8375777.500, Validation loss: 23273256.000\n",
            "[Epoch 12] Training loss: 5176174.500, Validation loss: 18243450.000\n",
            "[Epoch 13] Training loss: 6009349.000, Validation loss: 24559354.000\n",
            "[Epoch 14] Training loss: 7101031.500, Validation loss: 23869262.000\n",
            "[Epoch 15] Training loss: 7469147.500, Validation loss: 17731842.000\n",
            "[Epoch 16] Training loss: 6505981.500, Validation loss: 21390184.000\n",
            "[Epoch 17] Training loss: 4570903.000, Validation loss: 21304322.000\n",
            "[Epoch 18] Training loss: 4886492.500, Validation loss: 14604789.000\n",
            "[Epoch 19] Training loss: 4176199.250, Validation loss: 14917637.000\n",
            "[Epoch 20] Training loss: 10154807.000, Validation loss: 32923782.000\n",
            "[Epoch 21] Training loss: 8997101.000, Validation loss: 14600979.000\n",
            "[Epoch 22] Training loss: 5364686.500, Validation loss: 16392499.000\n",
            "[Epoch 23] Training loss: 8462635.000, Validation loss: 13652092.000\n",
            "[Epoch 24] Training loss: 4327801.500, Validation loss: 13414896.000\n",
            "[Epoch 25] Training loss: 6011362.500, Validation loss: 15774456.000\n",
            "[Epoch 26] Training loss: 6085803.000, Validation loss: 12503044.000\n",
            "[Epoch 27] Training loss: 9616821.000, Validation loss: 31086456.000\n",
            "[Epoch 28] Training loss: 8712006.000, Validation loss: 16007461.000\n",
            "[Epoch 29] Training loss: 6114210.500, Validation loss: 16777482.000\n",
            "[Epoch 30] Training loss: 5330768.000, Validation loss: 12390919.000\n",
            "[Epoch 31] Training loss: 5501840.500, Validation loss: 20558640.000\n",
            "[Epoch 32] Training loss: 12184128.000, Validation loss: 16030793.000\n",
            "[Epoch 33] Training loss: 10983792.000, Validation loss: 18378998.000\n",
            "[Epoch 34] Training loss: 5242944.500, Validation loss: 13138089.000\n",
            "[Epoch 35] Training loss: 3643932.750, Validation loss: 12560659.000\n",
            "[Epoch 36] Training loss: 5085058.000, Validation loss: 19645820.000\n",
            "[Epoch 37] Training loss: 5366985.500, Validation loss: 13419845.000\n",
            "[Epoch 38] Training loss: 6363576.000, Validation loss: 11876460.000\n",
            "[Epoch 39] Training loss: 4766276.500, Validation loss: 15665951.000\n",
            "[Epoch 40] Training loss: 3735759.500, Validation loss: 11465872.000\n",
            "[Epoch 41] Training loss: 3476108.750, Validation loss: 11272720.000\n",
            "[Epoch 42] Training loss: 6669418.000, Validation loss: 14961193.000\n",
            "[Epoch 43] Training loss: 4653075.000, Validation loss: 11418840.000\n",
            "[Epoch 44] Training loss: 3157873.250, Validation loss: 10928765.000\n",
            "[Epoch 45] Training loss: 3780313.000, Validation loss: 11949703.000\n",
            "[Epoch 46] Training loss: 5041032.500, Validation loss: 25579086.000\n",
            "[Epoch 47] Training loss: 6672056.000, Validation loss: 11467769.000\n",
            "[Epoch 48] Training loss: 5589301.500, Validation loss: 31615586.000\n",
            "[Epoch 49] Training loss: 8403141.000, Validation loss: 21082778.000\n",
            "[Epoch 50] Training loss: 4498426.500, Validation loss: 18425946.000\n",
            "Difference between the values of IRLS parameters in two successive iterations: 0.001464\n",
            "IRLS iteration 3 begins...\n",
            "tensor([[0.0688, 0.5419, 0.2123,  ..., 0.1055, 0.9338, 0.2519],\n",
            "        [0.2134, 0.2248, 0.0720,  ..., 0.0391, 0.1521, 0.1795],\n",
            "        [0.8545, 0.2687, 0.2397,  ..., 0.4327, 0.3245, 0.0772],\n",
            "        ...,\n",
            "        [0.2015, 0.4615, 0.3589,  ..., 0.8964, 0.0255, 0.3440],\n",
            "        [0.2582, 0.5866, 0.1213,  ..., 0.0533, 0.3447, 0.5854],\n",
            "        [0.4721, 0.9490, 0.7929,  ..., 0.6801, 0.3396, 0.7106]],\n",
            "       requires_grad=True)\n",
            "[Epoch 1] Training loss: 61865992.000, Validation loss: 50714688.000\n",
            "[Epoch 2] Training loss: 11368350.000, Validation loss: 11623173.000\n",
            "[Epoch 3] Training loss: 3823828.250, Validation loss: 10689841.000\n",
            "[Epoch 4] Training loss: 3341541.000, Validation loss: 11037645.000\n",
            "[Epoch 5] Training loss: 3056913.250, Validation loss: 12296328.000\n",
            "[Epoch 6] Training loss: 3220866.500, Validation loss: 19222454.000\n",
            "[Epoch 7] Training loss: 5581126.000, Validation loss: 17722330.000\n",
            "[Epoch 8] Training loss: 3843709.500, Validation loss: 10934845.000\n",
            "[Epoch 9] Training loss: 2383540.250, Validation loss: 10515265.000\n",
            "[Epoch 10] Training loss: 4409317.000, Validation loss: 10465761.000\n",
            "[Epoch 11] Training loss: 5412162.500, Validation loss: 11888672.000\n",
            "[Epoch 12] Training loss: 4474224.500, Validation loss: 19069978.000\n",
            "[Epoch 13] Training loss: 3983779.500, Validation loss: 25828896.000\n",
            "[Epoch 14] Training loss: 3390839.750, Validation loss: 10292692.000\n",
            "[Epoch 15] Training loss: 4015671.500, Validation loss: 16316069.000\n",
            "[Epoch 16] Training loss: 2920649.250, Validation loss: 10542457.000\n",
            "[Epoch 17] Training loss: 3881267.750, Validation loss: 14920596.000\n",
            "[Epoch 18] Training loss: 4887892.500, Validation loss: 11671295.000\n",
            "[Epoch 19] Training loss: 3616012.750, Validation loss: 10436369.000\n",
            "[Epoch 20] Training loss: 2727332.250, Validation loss: 13130308.000\n",
            "[Epoch 21] Training loss: 3452650.250, Validation loss: 12134152.000\n",
            "[Epoch 22] Training loss: 6000279.000, Validation loss: 12959288.000\n",
            "[Epoch 23] Training loss: 4253056.500, Validation loss: 10026432.000\n",
            "[Epoch 24] Training loss: 5911042.500, Validation loss: 13346311.000\n",
            "[Epoch 25] Training loss: 9418617.000, Validation loss: 30132762.000\n",
            "[Epoch 26] Training loss: 6930546.500, Validation loss: 10098512.000\n",
            "[Epoch 27] Training loss: 5169957.000, Validation loss: 11803656.000\n",
            "[Epoch 28] Training loss: 4057632.250, Validation loss: 9943081.000\n",
            "[Epoch 29] Training loss: 3703545.250, Validation loss: 11180179.000\n",
            "[Epoch 30] Training loss: 3718271.000, Validation loss: 16729829.000\n",
            "[Epoch 31] Training loss: 5178642.000, Validation loss: 17087238.000\n",
            "[Epoch 32] Training loss: 3999893.250, Validation loss: 13912736.000\n",
            "[Epoch 33] Training loss: 3019733.250, Validation loss: 13307035.000\n",
            "[Epoch 34] Training loss: 2663181.000, Validation loss: 9880491.000\n",
            "[Epoch 35] Training loss: 2429471.250, Validation loss: 12308713.000\n",
            "[Epoch 36] Training loss: 5682743.000, Validation loss: 10976308.000\n",
            "[Epoch 37] Training loss: 3850927.750, Validation loss: 12338611.000\n",
            "[Epoch 38] Training loss: 2621164.250, Validation loss: 9109236.000\n",
            "[Epoch 39] Training loss: 2883126.750, Validation loss: 9321803.000\n",
            "[Epoch 40] Training loss: 4822439.000, Validation loss: 15195053.000\n",
            "[Epoch 41] Training loss: 4123447.000, Validation loss: 9035349.000\n",
            "[Epoch 42] Training loss: 3758829.000, Validation loss: 9741675.000\n",
            "[Epoch 43] Training loss: 2230664.500, Validation loss: 9365469.000\n",
            "[Epoch 44] Training loss: 2614928.250, Validation loss: 9743275.000\n",
            "[Epoch 45] Training loss: 4098587.750, Validation loss: 9580952.000\n",
            "[Epoch 46] Training loss: 2957464.500, Validation loss: 9341556.000\n",
            "[Epoch 47] Training loss: 2631295.750, Validation loss: 9285064.000\n",
            "[Epoch 48] Training loss: 2655045.250, Validation loss: 8721988.000\n",
            "[Epoch 49] Training loss: 3563552.000, Validation loss: 9543210.000\n",
            "[Epoch 50] Training loss: 2746280.000, Validation loss: 9416917.000\n",
            "Difference between the values of IRLS parameters in two successive iterations: 0.001520\n",
            "IRLS iteration 4 begins...\n",
            "tensor([[0.0690, 0.5421, 0.2124,  ..., 0.1056, 0.9338, 0.2506],\n",
            "        [0.2135, 0.2249, 0.0721,  ..., 0.0406, 0.1538, 0.1797],\n",
            "        [0.8544, 0.2686, 0.2396,  ..., 0.4324, 0.3241, 0.0762],\n",
            "        ...,\n",
            "        [0.2013, 0.4612, 0.3586,  ..., 0.8950, 0.0248, 0.3489],\n",
            "        [0.2584, 0.5869, 0.1216,  ..., 0.0549, 0.3452, 0.5779],\n",
            "        [0.4722, 0.9492, 0.7930,  ..., 0.6806, 0.3402, 0.7111]],\n",
            "       requires_grad=True)\n",
            "[Epoch 1] Training loss: 71987536.000, Validation loss: 39991500.000\n",
            "[Epoch 2] Training loss: 15878141.000, Validation loss: 33323958.000\n",
            "[Epoch 3] Training loss: 7978739.500, Validation loss: 11843595.000\n",
            "[Epoch 4] Training loss: 4063388.000, Validation loss: 8423045.000\n",
            "[Epoch 5] Training loss: 2055688.375, Validation loss: 11200577.000\n",
            "[Epoch 6] Training loss: 1829655.875, Validation loss: 8348269.500\n",
            "[Epoch 7] Training loss: 2092532.500, Validation loss: 8480039.000\n",
            "[Epoch 8] Training loss: 2114669.000, Validation loss: 8342008.000\n",
            "[Epoch 9] Training loss: 1651118.625, Validation loss: 8330379.500\n",
            "[Epoch 10] Training loss: 1908288.625, Validation loss: 8405028.000\n",
            "[Epoch 11] Training loss: 2537614.250, Validation loss: 11045965.000\n",
            "[Epoch 12] Training loss: 2531615.750, Validation loss: 8427145.000\n",
            "[Epoch 13] Training loss: 2100970.250, Validation loss: 9369029.000\n",
            "[Epoch 14] Training loss: 2061425.375, Validation loss: 14386835.000\n",
            "[Epoch 15] Training loss: 2074938.000, Validation loss: 16753792.000\n",
            "[Epoch 16] Training loss: 3482781.500, Validation loss: 8298854.000\n",
            "[Epoch 17] Training loss: 4027696.500, Validation loss: 11475767.000\n",
            "[Epoch 18] Training loss: 3972542.500, Validation loss: 8278761.500\n",
            "[Epoch 19] Training loss: 2223155.500, Validation loss: 8402836.000\n",
            "[Epoch 20] Training loss: 2794051.750, Validation loss: 8363708.500\n",
            "[Epoch 21] Training loss: 1793346.500, Validation loss: 9759039.000\n",
            "[Epoch 22] Training loss: 2506240.500, Validation loss: 8921055.000\n",
            "[Epoch 23] Training loss: 4579247.500, Validation loss: 8380468.000\n",
            "[Epoch 24] Training loss: 4100337.750, Validation loss: 10516219.000\n",
            "[Epoch 25] Training loss: 4349786.500, Validation loss: 8824495.000\n",
            "[Epoch 26] Training loss: 2692977.000, Validation loss: 9949860.000\n",
            "[Epoch 27] Training loss: 2379092.500, Validation loss: 8498231.000\n",
            "[Epoch 28] Training loss: 2811418.750, Validation loss: 8842455.000\n",
            "[Epoch 29] Training loss: 2930434.250, Validation loss: 12163888.000\n",
            "[Epoch 30] Training loss: 3960165.250, Validation loss: 8453574.000\n",
            "[Epoch 31] Training loss: 2772427.000, Validation loss: 8206814.500\n",
            "[Epoch 32] Training loss: 2201009.750, Validation loss: 10170760.000\n",
            "[Epoch 33] Training loss: 5071109.500, Validation loss: 8441197.000\n",
            "[Epoch 34] Training loss: 5401239.500, Validation loss: 13938429.000\n",
            "[Epoch 35] Training loss: 3277098.250, Validation loss: 9145012.000\n",
            "[Epoch 36] Training loss: 1928179.625, Validation loss: 8174676.500\n",
            "[Epoch 37] Training loss: 1580889.125, Validation loss: 10578702.000\n",
            "[Epoch 38] Training loss: 2256995.750, Validation loss: 8684685.000\n",
            "[Epoch 39] Training loss: 2890810.500, Validation loss: 7841674.500\n",
            "[Epoch 40] Training loss: 1994464.125, Validation loss: 10052091.000\n",
            "[Epoch 41] Training loss: 2565250.750, Validation loss: 8109259.500\n",
            "[Epoch 42] Training loss: 1592347.750, Validation loss: 9609117.000\n",
            "[Epoch 43] Training loss: 2233581.750, Validation loss: 7702153.500\n",
            "[Epoch 44] Training loss: 3223131.500, Validation loss: 9297317.000\n",
            "[Epoch 45] Training loss: 2784274.250, Validation loss: 8271212.000\n",
            "[Epoch 46] Training loss: 2027156.125, Validation loss: 7627270.500\n",
            "[Epoch 47] Training loss: 2575276.000, Validation loss: 8073908.000\n",
            "[Epoch 48] Training loss: 2188792.500, Validation loss: 7603820.000\n",
            "[Epoch 49] Training loss: 3719092.500, Validation loss: 10454964.000\n",
            "[Epoch 50] Training loss: 3154167.750, Validation loss: 10508035.000\n",
            "Difference between the values of IRLS parameters in two successive iterations: 0.001231\n",
            "IRLS iteration 5 begins...\n",
            "tensor([[0.0691, 0.5422, 0.2126,  ..., 0.1057, 0.9337, 0.2494],\n",
            "        [0.2136, 0.2250, 0.0722,  ..., 0.0417, 0.1549, 0.1796],\n",
            "        [0.8543, 0.2685, 0.2394,  ..., 0.4321, 0.3237, 0.0753],\n",
            "        ...,\n",
            "        [0.2011, 0.4610, 0.3584,  ..., 0.8941, 0.0246, 0.3536],\n",
            "        [0.2586, 0.5871, 0.1217,  ..., 0.0562, 0.3454, 0.5707],\n",
            "        [0.4723, 0.9493, 0.7931,  ..., 0.6810, 0.3406, 0.7115]],\n",
            "       requires_grad=True)\n",
            "[Epoch 1] Training loss: 60611400.000, Validation loss: 48004172.000\n",
            "[Epoch 2] Training loss: 12370658.000, Validation loss: 16132581.000\n",
            "[Epoch 3] Training loss: 3340273.000, Validation loss: 12864547.000\n",
            "[Epoch 4] Training loss: 2694982.250, Validation loss: 7663174.500\n",
            "[Epoch 5] Training loss: 1226624.000, Validation loss: 7664660.000\n",
            "[Epoch 6] Training loss: 1489670.625, Validation loss: 7692928.000\n",
            "[Epoch 7] Training loss: 1540940.625, Validation loss: 7609698.500\n",
            "[Epoch 8] Training loss: 2278382.500, Validation loss: 7819304.500\n",
            "[Epoch 9] Training loss: 1605540.750, Validation loss: 8866991.000\n",
            "[Epoch 10] Training loss: 1981930.250, Validation loss: 8148541.500\n",
            "[Epoch 11] Training loss: 1563632.750, Validation loss: 7419090.500\n",
            "[Epoch 12] Training loss: 1386930.375, Validation loss: 7436336.000\n",
            "[Epoch 13] Training loss: 1687691.750, Validation loss: 8033606.000\n",
            "[Epoch 14] Training loss: 1826128.125, Validation loss: 10217029.000\n",
            "[Epoch 15] Training loss: 1828367.125, Validation loss: 10016135.000\n",
            "[Epoch 16] Training loss: 2191461.750, Validation loss: 7943107.500\n",
            "[Epoch 17] Training loss: 3773478.500, Validation loss: 8905581.000\n",
            "[Epoch 18] Training loss: 8399143.000, Validation loss: 7227110.000\n",
            "[Epoch 19] Training loss: 3436377.500, Validation loss: 8194709.500\n",
            "[Epoch 20] Training loss: 2501989.500, Validation loss: 7206856.500\n",
            "[Epoch 21] Training loss: 1714163.875, Validation loss: 7549594.500\n",
            "[Epoch 22] Training loss: 1212212.250, Validation loss: 7488754.500\n",
            "[Epoch 23] Training loss: 1329684.500, Validation loss: 11758897.000\n",
            "[Epoch 24] Training loss: 2212501.250, Validation loss: 8515585.000\n",
            "[Epoch 25] Training loss: 4385757.500, Validation loss: 7097506.500\n",
            "[Epoch 26] Training loss: 3178395.000, Validation loss: 10071133.000\n",
            "[Epoch 27] Training loss: 2304429.750, Validation loss: 8935501.000\n",
            "[Epoch 28] Training loss: 2007011.500, Validation loss: 12745867.000\n",
            "[Epoch 29] Training loss: 2873449.500, Validation loss: 12514267.000\n",
            "[Epoch 30] Training loss: 4253325.500, Validation loss: 8610711.000\n",
            "[Epoch 31] Training loss: 1616554.125, Validation loss: 6801945.500\n",
            "[Epoch 32] Training loss: 1039597.875, Validation loss: 7358802.500\n",
            "[Epoch 33] Training loss: 1151985.500, Validation loss: 6886776.000\n",
            "[Epoch 34] Training loss: 1335461.500, Validation loss: 6893830.500\n",
            "[Epoch 35] Training loss: 2001291.500, Validation loss: 6868376.000\n",
            "[Epoch 36] Training loss: 1920314.625, Validation loss: 6959602.500\n",
            "[Epoch 37] Training loss: 2882572.750, Validation loss: 8289170.500\n",
            "[Epoch 38] Training loss: 7557062.000, Validation loss: 25254200.000\n",
            "[Epoch 39] Training loss: 5218968.000, Validation loss: 10829887.000\n",
            "[Epoch 40] Training loss: 1482882.750, Validation loss: 8451807.000\n",
            "[Epoch 41] Training loss: 2122087.000, Validation loss: 14026169.000\n",
            "[Epoch 42] Training loss: 2454017.500, Validation loss: 9360644.000\n",
            "[Epoch 43] Training loss: 3170494.500, Validation loss: 8002410.000\n",
            "[Epoch 44] Training loss: 1627733.625, Validation loss: 6846868.500\n",
            "[Epoch 45] Training loss: 1608206.875, Validation loss: 7170301.500\n",
            "[Epoch 46] Training loss: 2443650.750, Validation loss: 12052356.000\n",
            "[Epoch 47] Training loss: 2566225.250, Validation loss: 10803179.000\n",
            "[Epoch 48] Training loss: 1433387.875, Validation loss: 6754640.500\n",
            "[Epoch 49] Training loss: 1334615.500, Validation loss: 7778584.000\n",
            "[Epoch 50] Training loss: 1687360.125, Validation loss: 6615888.500\n",
            "Difference between the values of IRLS parameters in two successive iterations: 0.001008\n",
            "IRLS iteration 6 begins...\n",
            "tensor([[0.0692, 0.5423, 0.2126,  ..., 0.1058, 0.9337, 0.2484],\n",
            "        [0.2136, 0.2251, 0.0723,  ..., 0.0428, 0.1561, 0.1798],\n",
            "        [0.8543, 0.2685, 0.2394,  ..., 0.4319, 0.3234, 0.0746],\n",
            "        ...,\n",
            "        [0.2010, 0.4610, 0.3584,  ..., 0.8928, 0.0238, 0.3573],\n",
            "        [0.2587, 0.5871, 0.1218,  ..., 0.0584, 0.3464, 0.5638],\n",
            "        [0.4724, 0.9494, 0.7932,  ..., 0.6814, 0.3410, 0.7119]],\n",
            "       requires_grad=True)\n",
            "[Epoch 1] Training loss: 72307496.000, Validation loss: 56321332.000\n",
            "[Epoch 2] Training loss: 13924705.000, Validation loss: 24341618.000\n",
            "[Epoch 3] Training loss: 4563670.000, Validation loss: 8095467.500\n",
            "[Epoch 4] Training loss: 2501888.250, Validation loss: 12043597.000\n",
            "[Epoch 5] Training loss: 2046926.875, Validation loss: 6612689.500\n",
            "[Epoch 6] Training loss: 1134524.750, Validation loss: 6667417.500\n",
            "[Epoch 7] Training loss: 1614235.125, Validation loss: 11374356.000\n",
            "[Epoch 8] Training loss: 1791804.500, Validation loss: 8425053.000\n",
            "[Epoch 9] Training loss: 1269680.125, Validation loss: 7382494.000\n",
            "[Epoch 10] Training loss: 1625775.500, Validation loss: 6635580.500\n",
            "[Epoch 11] Training loss: 1198575.375, Validation loss: 6538393.500\n",
            "[Epoch 12] Training loss: 974787.625, Validation loss: 7545461.500\n",
            "[Epoch 13] Training loss: 1679344.625, Validation loss: 6623639.500\n",
            "[Epoch 14] Training loss: 974909.000, Validation loss: 6638796.500\n",
            "[Epoch 15] Training loss: 991823.250, Validation loss: 6633385.500\n",
            "[Epoch 16] Training loss: 1530577.875, Validation loss: 6580656.500\n",
            "[Epoch 17] Training loss: 1321674.000, Validation loss: 6913286.000\n",
            "[Epoch 18] Training loss: 1544499.250, Validation loss: 13414885.000\n",
            "[Epoch 19] Training loss: 1771116.625, Validation loss: 6877989.500\n",
            "[Epoch 20] Training loss: 1687142.000, Validation loss: 6629906.000\n",
            "[Epoch 21] Training loss: 1816719.750, Validation loss: 7665513.500\n",
            "[Epoch 22] Training loss: 1213677.125, Validation loss: 6695996.000\n",
            "[Epoch 23] Training loss: 1024937.438, Validation loss: 8436198.000\n",
            "[Epoch 24] Training loss: 1177407.750, Validation loss: 6519414.000\n",
            "[Epoch 25] Training loss: 1117095.625, Validation loss: 6515317.500\n",
            "[Epoch 26] Training loss: 1564883.750, Validation loss: 6820572.500\n",
            "[Epoch 27] Training loss: 3420788.250, Validation loss: 18239090.000\n",
            "[Epoch 28] Training loss: 8050824.500, Validation loss: 8632687.000\n",
            "[Epoch 29] Training loss: 5806288.000, Validation loss: 8350785.500\n",
            "[Epoch 30] Training loss: 2952002.750, Validation loss: 6637134.000\n",
            "[Epoch 31] Training loss: 2497639.500, Validation loss: 8999013.000\n",
            "[Epoch 32] Training loss: 2445820.000, Validation loss: 7559419.500\n",
            "[Epoch 33] Training loss: 2561832.750, Validation loss: 7696505.500\n",
            "[Epoch 34] Training loss: 1052461.125, Validation loss: 6577741.500\n",
            "[Epoch 35] Training loss: 2182028.500, Validation loss: 8131623.500\n",
            "[Epoch 36] Training loss: 1800326.625, Validation loss: 6373954.000\n",
            "[Epoch 37] Training loss: 1506756.250, Validation loss: 8520528.000\n",
            "[Epoch 38] Training loss: 1911203.875, Validation loss: 6684977.500\n",
            "[Epoch 39] Training loss: 1072227.625, Validation loss: 6298506.000\n",
            "[Epoch 40] Training loss: 1376672.625, Validation loss: 6307037.500\n",
            "[Epoch 41] Training loss: 1012078.250, Validation loss: 6877758.000\n",
            "[Epoch 42] Training loss: 866116.375, Validation loss: 6295138.000\n",
            "[Epoch 43] Training loss: 909436.938, Validation loss: 7836728.000\n",
            "[Epoch 44] Training loss: 1567564.000, Validation loss: 8974494.000\n",
            "[Epoch 45] Training loss: 1415715.875, Validation loss: 7446217.500\n",
            "[Epoch 46] Training loss: 1199408.375, Validation loss: 6395590.000\n",
            "[Epoch 47] Training loss: 1664804.375, Validation loss: 9086654.000\n",
            "[Epoch 48] Training loss: 1693974.750, Validation loss: 8175292.000\n",
            "[Epoch 49] Training loss: 1461196.000, Validation loss: 6142458.500\n",
            "[Epoch 50] Training loss: 932816.312, Validation loss: 7409816.000\n",
            "Difference between the values of IRLS parameters in two successive iterations: 0.000756\n",
            "IRLS iteration 7 begins...\n",
            "tensor([[0.0693, 0.5424, 0.2127,  ..., 0.1058, 0.9336, 0.2477],\n",
            "        [0.2137, 0.2252, 0.0723,  ..., 0.0433, 0.1566, 0.1795],\n",
            "        [0.8542, 0.2684, 0.2393,  ..., 0.4318, 0.3232, 0.0740],\n",
            "        ...,\n",
            "        [0.2009, 0.4609, 0.3583,  ..., 0.8916, 0.0230, 0.3597],\n",
            "        [0.2588, 0.5872, 0.1219,  ..., 0.0597, 0.3470, 0.5596],\n",
            "        [0.4725, 0.9494, 0.7933,  ..., 0.6816, 0.3414, 0.7122]],\n",
            "       requires_grad=True)\n",
            "[Epoch 1] Training loss: 66892116.000, Validation loss: 49541420.000\n",
            "[Epoch 2] Training loss: 7791885.000, Validation loss: 13655731.000\n",
            "[Epoch 3] Training loss: 3358171.250, Validation loss: 8606394.000\n",
            "[Epoch 4] Training loss: 1162528.875, Validation loss: 6029264.000\n",
            "[Epoch 5] Training loss: 797098.875, Validation loss: 6265174.500\n",
            "[Epoch 6] Training loss: 931619.188, Validation loss: 6358865.500\n",
            "[Epoch 7] Training loss: 951981.625, Validation loss: 6214100.500\n",
            "[Epoch 8] Training loss: 1170552.625, Validation loss: 8406467.000\n",
            "[Epoch 9] Training loss: 1425513.625, Validation loss: 7124284.000\n",
            "[Epoch 10] Training loss: 906092.750, Validation loss: 8241552.500\n",
            "[Epoch 11] Training loss: 1309772.750, Validation loss: 6191351.500\n",
            "[Epoch 12] Training loss: 832058.562, Validation loss: 6066674.500\n",
            "[Epoch 13] Training loss: 1015762.875, Validation loss: 6643184.500\n",
            "[Epoch 14] Training loss: 1039107.062, Validation loss: 6069014.000\n",
            "[Epoch 15] Training loss: 1909315.125, Validation loss: 6547192.000\n",
            "[Epoch 16] Training loss: 1566380.625, Validation loss: 6224110.500\n",
            "[Epoch 17] Training loss: 1285206.875, Validation loss: 6478558.500\n",
            "[Epoch 18] Training loss: 941434.188, Validation loss: 7666998.500\n",
            "[Epoch 19] Training loss: 945506.312, Validation loss: 6414019.500\n",
            "[Epoch 20] Training loss: 965294.812, Validation loss: 6278193.500\n",
            "[Epoch 21] Training loss: 797919.562, Validation loss: 6157848.000\n",
            "[Epoch 22] Training loss: 802917.688, Validation loss: 6136434.500\n",
            "[Epoch 23] Training loss: 1263639.250, Validation loss: 6164184.500\n",
            "[Epoch 24] Training loss: 1964380.375, Validation loss: 14606133.000\n",
            "[Epoch 25] Training loss: 2554453.750, Validation loss: 6217512.000\n",
            "[Epoch 26] Training loss: 1300613.625, Validation loss: 6165822.500\n",
            "[Epoch 27] Training loss: 2303884.750, Validation loss: 6068286.500\n",
            "[Epoch 28] Training loss: 1549394.250, Validation loss: 10911345.000\n",
            "[Epoch 29] Training loss: 1364505.875, Validation loss: 6850585.500\n",
            "[Epoch 30] Training loss: 1540312.000, Validation loss: 6431272.500\n",
            "[Epoch 31] Training loss: 812881.875, Validation loss: 6247198.500\n",
            "[Epoch 32] Training loss: 1255448.375, Validation loss: 7937002.000\n",
            "[Epoch 33] Training loss: 1115028.250, Validation loss: 6821068.500\n",
            "[Epoch 34] Training loss: 911235.625, Validation loss: 6188979.500\n",
            "[Epoch 35] Training loss: 856257.875, Validation loss: 6362971.500\n",
            "[Epoch 36] Training loss: 1235488.000, Validation loss: 6131534.500\n",
            "[Epoch 37] Training loss: 1308142.250, Validation loss: 6379605.500\n",
            "[Epoch 38] Training loss: 1384926.875, Validation loss: 8195200.000\n",
            "[Epoch 39] Training loss: 1792179.875, Validation loss: 5916482.500\n",
            "[Epoch 40] Training loss: 1745288.875, Validation loss: 6550315.500\n",
            "[Epoch 41] Training loss: 4255026.000, Validation loss: 8002104.000\n",
            "[Epoch 42] Training loss: 2035087.750, Validation loss: 6324660.500\n",
            "[Epoch 43] Training loss: 1129743.750, Validation loss: 6094222.000\n",
            "[Epoch 44] Training loss: 1866951.750, Validation loss: 12114219.000\n",
            "[Epoch 45] Training loss: 3342113.750, Validation loss: 5829582.500\n",
            "[Epoch 46] Training loss: 908448.562, Validation loss: 5986878.500\n",
            "[Epoch 47] Training loss: 837223.250, Validation loss: 6206728.500\n",
            "[Epoch 48] Training loss: 814321.625, Validation loss: 5945325.500\n",
            "[Epoch 49] Training loss: 1543304.875, Validation loss: 9055771.000\n",
            "[Epoch 50] Training loss: 2174636.500, Validation loss: 7567961.500\n",
            "Difference between the values of IRLS parameters in two successive iterations: 0.000738\n",
            "IRLS iteration 8 begins...\n",
            "tensor([[0.0694, 0.5425, 0.2129,  ..., 0.1059, 0.9336, 0.2468],\n",
            "        [0.2138, 0.2253, 0.0724,  ..., 0.0439, 0.1573, 0.1792],\n",
            "        [0.8541, 0.2683, 0.2392,  ..., 0.4317, 0.3230, 0.0735],\n",
            "        ...,\n",
            "        [0.2008, 0.4607, 0.3581,  ..., 0.8908, 0.0227, 0.3629],\n",
            "        [0.2589, 0.5874, 0.1221,  ..., 0.0605, 0.3470, 0.5538],\n",
            "        [0.4726, 0.9495, 0.7934,  ..., 0.6821, 0.3418, 0.7126]],\n",
            "       requires_grad=True)\n",
            "[Epoch 1] Training loss: 61276600.000, Validation loss: 45677856.000\n",
            "[Epoch 2] Training loss: 11168010.000, Validation loss: 12860880.000\n",
            "[Epoch 3] Training loss: 2692625.500, Validation loss: 5651312.500\n",
            "[Epoch 4] Training loss: 822400.125, Validation loss: 6030286.500\n",
            "[Epoch 5] Training loss: 976956.625, Validation loss: 5870317.500\n",
            "[Epoch 6] Training loss: 865833.812, Validation loss: 7677524.500\n",
            "[Epoch 7] Training loss: 1072480.125, Validation loss: 6845542.500\n",
            "[Epoch 8] Training loss: 793468.875, Validation loss: 5855016.500\n",
            "[Epoch 9] Training loss: 1052764.375, Validation loss: 6290548.500\n",
            "[Epoch 10] Training loss: 905238.062, Validation loss: 6054429.500\n",
            "[Epoch 11] Training loss: 1143675.875, Validation loss: 6480524.500\n",
            "[Epoch 12] Training loss: 876030.562, Validation loss: 6701510.000\n",
            "[Epoch 13] Training loss: 1449598.625, Validation loss: 6205526.500\n",
            "[Epoch 14] Training loss: 678386.188, Validation loss: 6826293.500\n",
            "[Epoch 15] Training loss: 838813.312, Validation loss: 6589322.500\n",
            "[Epoch 16] Training loss: 1927727.625, Validation loss: 6936417.500\n",
            "[Epoch 17] Training loss: 1711956.500, Validation loss: 5748826.000\n",
            "[Epoch 18] Training loss: 969254.688, Validation loss: 6105541.500\n",
            "[Epoch 19] Training loss: 1255022.125, Validation loss: 6444244.000\n",
            "[Epoch 20] Training loss: 983764.875, Validation loss: 5722776.000\n",
            "[Epoch 21] Training loss: 993971.250, Validation loss: 6141186.000\n",
            "[Epoch 22] Training loss: 1176717.250, Validation loss: 5786246.500\n",
            "[Epoch 23] Training loss: 837980.688, Validation loss: 5892692.500\n",
            "[Epoch 24] Training loss: 779444.375, Validation loss: 7385494.500\n",
            "[Epoch 25] Training loss: 709299.688, Validation loss: 5677530.000\n",
            "[Epoch 26] Training loss: 1907582.500, Validation loss: 7140993.500\n",
            "[Epoch 27] Training loss: 1037382.250, Validation loss: 8318490.000\n",
            "[Epoch 28] Training loss: 2638905.500, Validation loss: 5970793.500\n",
            "[Epoch 29] Training loss: 959111.312, Validation loss: 5774490.000\n",
            "[Epoch 30] Training loss: 1042348.812, Validation loss: 5795555.500\n",
            "[Epoch 31] Training loss: 745118.250, Validation loss: 5912448.000\n",
            "[Epoch 32] Training loss: 2175898.250, Validation loss: 5745440.000\n",
            "[Epoch 33] Training loss: 2025357.750, Validation loss: 8736150.000\n",
            "[Epoch 34] Training loss: 2853687.000, Validation loss: 6347917.500\n",
            "[Epoch 35] Training loss: 1618140.250, Validation loss: 8249847.500\n",
            "[Epoch 36] Training loss: 1190942.125, Validation loss: 6165620.500\n",
            "[Epoch 37] Training loss: 1221008.625, Validation loss: 6141246.000\n",
            "[Epoch 38] Training loss: 904829.125, Validation loss: 5606332.500\n",
            "[Epoch 39] Training loss: 1263559.125, Validation loss: 9311883.000\n",
            "[Epoch 40] Training loss: 1388602.250, Validation loss: 5652807.500\n",
            "[Epoch 41] Training loss: 1251283.875, Validation loss: 7252804.500\n",
            "[Epoch 42] Training loss: 1989687.875, Validation loss: 5632234.500\n",
            "[Epoch 43] Training loss: 1374575.750, Validation loss: 6543692.500\n",
            "[Epoch 44] Training loss: 1923208.500, Validation loss: 8377742.500\n",
            "[Epoch 45] Training loss: 1310202.375, Validation loss: 6755965.500\n",
            "[Epoch 46] Training loss: 1166666.875, Validation loss: 5514648.000\n",
            "[Epoch 47] Training loss: 2160493.500, Validation loss: 9663469.000\n",
            "[Epoch 48] Training loss: 2705820.250, Validation loss: 6825884.500\n",
            "[Epoch 49] Training loss: 1981196.625, Validation loss: 5597261.500\n",
            "[Epoch 50] Training loss: 1052271.500, Validation loss: 5731211.500\n",
            "Difference between the values of IRLS parameters in two successive iterations: 0.000551\n",
            "IRLS iteration 9 begins...\n",
            "tensor([[0.0695, 0.5426, 0.2129,  ..., 0.1059, 0.9335, 0.2462],\n",
            "        [0.2139, 0.2253, 0.0725,  ..., 0.0443, 0.1578, 0.1791],\n",
            "        [0.8540, 0.2682, 0.2391,  ..., 0.4315, 0.3228, 0.0730],\n",
            "        ...,\n",
            "        [0.2008, 0.4607, 0.3581,  ..., 0.8903, 0.0225, 0.3654],\n",
            "        [0.2589, 0.5874, 0.1221,  ..., 0.0610, 0.3470, 0.5490],\n",
            "        [0.4726, 0.9496, 0.7935,  ..., 0.6823, 0.3421, 0.7128]],\n",
            "       requires_grad=True)\n",
            "[Epoch 1] Training loss: 70888184.000, Validation loss: 57220684.000\n",
            "[Epoch 2] Training loss: 12703852.000, Validation loss: 16539877.000\n",
            "[Epoch 3] Training loss: 4408333.500, Validation loss: 7441241.500\n",
            "[Epoch 4] Training loss: 1361259.625, Validation loss: 5691699.500\n",
            "[Epoch 5] Training loss: 504632.656, Validation loss: 5278905.500\n",
            "[Epoch 6] Training loss: 593415.312, Validation loss: 5448269.500\n",
            "[Epoch 7] Training loss: 535801.750, Validation loss: 5330920.500\n",
            "[Epoch 8] Training loss: 673171.938, Validation loss: 5806340.500\n",
            "[Epoch 9] Training loss: 750358.812, Validation loss: 6120266.000\n",
            "[Epoch 10] Training loss: 620191.688, Validation loss: 5436211.000\n",
            "[Epoch 11] Training loss: 829392.188, Validation loss: 6328535.500\n",
            "[Epoch 12] Training loss: 1049043.500, Validation loss: 5312259.000\n",
            "[Epoch 13] Training loss: 825487.375, Validation loss: 5420960.500\n",
            "[Epoch 14] Training loss: 780371.000, Validation loss: 5529032.000\n",
            "[Epoch 15] Training loss: 696509.562, Validation loss: 6192991.500\n",
            "[Epoch 16] Training loss: 816203.438, Validation loss: 5382197.500\n",
            "[Epoch 17] Training loss: 573033.375, Validation loss: 5516608.500\n",
            "[Epoch 18] Training loss: 1312505.125, Validation loss: 7471638.000\n",
            "[Epoch 19] Training loss: 3053307.000, Validation loss: 10900135.000\n",
            "[Epoch 20] Training loss: 3438333.750, Validation loss: 8236941.500\n",
            "[Epoch 21] Training loss: 3105825.250, Validation loss: 5496061.500\n",
            "[Epoch 22] Training loss: 1156790.375, Validation loss: 6041011.500\n",
            "[Epoch 23] Training loss: 604502.750, Validation loss: 5698920.500\n",
            "[Epoch 24] Training loss: 723899.000, Validation loss: 6101957.500\n",
            "[Epoch 25] Training loss: 870979.688, Validation loss: 6720654.000\n",
            "[Epoch 26] Training loss: 1155028.250, Validation loss: 5426528.000\n",
            "[Epoch 27] Training loss: 805491.688, Validation loss: 5605462.000\n",
            "[Epoch 28] Training loss: 512805.125, Validation loss: 5413460.500\n",
            "[Epoch 29] Training loss: 781655.062, Validation loss: 5721906.000\n",
            "[Epoch 30] Training loss: 549405.250, Validation loss: 5544944.000\n",
            "[Epoch 31] Training loss: 860654.125, Validation loss: 7013383.500\n",
            "[Epoch 32] Training loss: 1459262.500, Validation loss: 5893610.500\n",
            "[Epoch 33] Training loss: 720534.875, Validation loss: 5340110.500\n",
            "[Epoch 34] Training loss: 1158427.375, Validation loss: 5735896.500\n",
            "[Epoch 35] Training loss: 789321.875, Validation loss: 5297297.500\n",
            "[Epoch 36] Training loss: 776024.438, Validation loss: 8461894.000\n",
            "[Epoch 37] Training loss: 2449743.750, Validation loss: 10589417.000\n",
            "[Epoch 38] Training loss: 1379562.750, Validation loss: 5277666.000\n",
            "[Epoch 39] Training loss: 825309.000, Validation loss: 5162765.000\n",
            "[Epoch 40] Training loss: 942697.188, Validation loss: 6003267.500\n",
            "[Epoch 41] Training loss: 1603013.125, Validation loss: 7459651.500\n",
            "[Epoch 42] Training loss: 2259026.250, Validation loss: 11912832.000\n",
            "[Epoch 43] Training loss: 3100952.500, Validation loss: 6736694.000\n",
            "[Epoch 44] Training loss: 2019508.625, Validation loss: 5379198.000\n",
            "[Epoch 45] Training loss: 1984040.500, Validation loss: 5727642.500\n",
            "[Epoch 46] Training loss: 1038975.062, Validation loss: 5332671.000\n",
            "[Epoch 47] Training loss: 726467.000, Validation loss: 5850360.500\n",
            "[Epoch 48] Training loss: 820917.625, Validation loss: 7355400.500\n",
            "[Epoch 49] Training loss: 1608154.125, Validation loss: 6741648.000\n",
            "[Epoch 50] Training loss: 1097754.375, Validation loss: 5367324.500\n",
            "Difference between the values of IRLS parameters in two successive iterations: 0.000630\n",
            "IRLS iteration 10 begins...\n",
            "tensor([[0.0695, 0.5426, 0.2129,  ..., 0.1059, 0.9334, 0.2456],\n",
            "        [0.2139, 0.2254, 0.0725,  ..., 0.0450, 0.1585, 0.1790],\n",
            "        [0.8540, 0.2682, 0.2391,  ..., 0.4314, 0.3227, 0.0726],\n",
            "        ...,\n",
            "        [0.2008, 0.4607, 0.3581,  ..., 0.8898, 0.0223, 0.3677],\n",
            "        [0.2589, 0.5874, 0.1221,  ..., 0.0620, 0.3474, 0.5450],\n",
            "        [0.4726, 0.9496, 0.7935,  ..., 0.6825, 0.3423, 0.7130]],\n",
            "       requires_grad=True)\n",
            "[Epoch 1] Training loss: 70216896.000, Validation loss: 60288844.000\n",
            "[Epoch 2] Training loss: 12687787.000, Validation loss: 16520959.000\n",
            "[Epoch 3] Training loss: 3463879.250, Validation loss: 9318473.000\n",
            "[Epoch 4] Training loss: 1184809.625, Validation loss: 5684574.000\n",
            "[Epoch 5] Training loss: 446923.656, Validation loss: 5742092.000\n",
            "[Epoch 6] Training loss: 559089.000, Validation loss: 5408015.500\n",
            "[Epoch 7] Training loss: 483926.000, Validation loss: 5343060.000\n",
            "[Epoch 8] Training loss: 717378.625, Validation loss: 5383596.500\n",
            "[Epoch 9] Training loss: 477123.531, Validation loss: 5420001.000\n",
            "[Epoch 10] Training loss: 499684.094, Validation loss: 5351078.000\n",
            "[Epoch 11] Training loss: 485723.062, Validation loss: 5331583.500\n",
            "[Epoch 12] Training loss: 616433.062, Validation loss: 5859585.500\n",
            "[Epoch 13] Training loss: 819413.812, Validation loss: 5783337.500\n",
            "[Epoch 14] Training loss: 449496.188, Validation loss: 5257683.500\n",
            "[Epoch 15] Training loss: 409726.469, Validation loss: 6645220.000\n",
            "[Epoch 16] Training loss: 780259.625, Validation loss: 5313265.500\n",
            "[Epoch 17] Training loss: 443588.906, Validation loss: 5426077.500\n",
            "[Epoch 18] Training loss: 491400.469, Validation loss: 6085450.500\n",
            "[Epoch 19] Training loss: 514808.062, Validation loss: 5518077.500\n",
            "[Epoch 20] Training loss: 553794.438, Validation loss: 5829509.500\n",
            "[Epoch 21] Training loss: 457094.938, Validation loss: 5283056.000\n",
            "[Epoch 22] Training loss: 735239.062, Validation loss: 5359306.500\n",
            "[Epoch 23] Training loss: 562629.000, Validation loss: 5290668.500\n",
            "[Epoch 24] Training loss: 509977.344, Validation loss: 5306991.500\n",
            "[Epoch 25] Training loss: 876138.562, Validation loss: 5248124.500\n",
            "[Epoch 26] Training loss: 415584.031, Validation loss: 5605149.500\n",
            "[Epoch 27] Training loss: 1023797.000, Validation loss: 5264696.500\n",
            "[Epoch 28] Training loss: 445590.469, Validation loss: 5506109.000\n",
            "[Epoch 29] Training loss: 681763.875, Validation loss: 6371990.000\n",
            "[Epoch 30] Training loss: 804785.188, Validation loss: 7165540.000\n",
            "[Epoch 31] Training loss: 874094.188, Validation loss: 5292139.500\n",
            "[Epoch 32] Training loss: 1090336.500, Validation loss: 5593904.000\n",
            "[Epoch 33] Training loss: 850220.938, Validation loss: 6049350.500\n",
            "[Epoch 34] Training loss: 876533.000, Validation loss: 6314540.500\n",
            "[Epoch 35] Training loss: 842143.375, Validation loss: 5282653.500\n",
            "[Epoch 36] Training loss: 411819.812, Validation loss: 5424828.500\n",
            "[Epoch 37] Training loss: 797838.125, Validation loss: 5512280.500\n",
            "[Epoch 38] Training loss: 403807.562, Validation loss: 6373121.500\n",
            "[Epoch 39] Training loss: 1239565.375, Validation loss: 7811670.000\n",
            "[Epoch 40] Training loss: 1570594.875, Validation loss: 5361417.500\n",
            "[Epoch 41] Training loss: 500858.500, Validation loss: 5364649.500\n",
            "[Epoch 42] Training loss: 663517.750, Validation loss: 5337404.000\n",
            "[Epoch 43] Training loss: 794118.562, Validation loss: 5442659.000\n",
            "[Epoch 44] Training loss: 1512105.750, Validation loss: 5268047.500\n",
            "[Epoch 45] Training loss: 561510.250, Validation loss: 6891950.500\n",
            "[Epoch 46] Training loss: 686038.188, Validation loss: 5683784.000\n",
            "[Epoch 47] Training loss: 490704.625, Validation loss: 5458089.500\n",
            "[Epoch 48] Training loss: 625075.625, Validation loss: 6686696.500\n",
            "[Epoch 49] Training loss: 852074.312, Validation loss: 6697100.000\n",
            "[Epoch 50] Training loss: 1181927.375, Validation loss: 5755481.500\n",
            "Difference between the values of IRLS parameters in two successive iterations: 0.000446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQOoToBIcuXr"
      },
      "source": [
        "# Using GPU device\n",
        "In the above example, we have not used GPUs. [Here's an example](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_autograd.html) as to how you may modify the above code to allow the code to make use of GPUs when they are available. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHjuEEN-dUqa"
      },
      "source": [
        "# Using tensor.detach() and tensor.clone()\n",
        "[Here's a nice post](https://medium.com/@attyuttam/5-gradient-derivative-related-pytorch-functions-8fd0e02f13c6#:~:text=You%20should%20use%20detach(),recorded%20as%20a%20directed%20graph.) on several commonly used PyTorch functions that are related to calculations of gradient/derivative, including:\n",
        "- detach()\n",
        "- no_grad()\n",
        "- clone()\n",
        "- backward()\n",
        "- register_hook()\n",
        "\n",
        "I've extracted a few keypoints here as follows:\n",
        "\n",
        "1. detach() creates a tensor that shares storage with tensor that does not require grad. You should use detach() when attempting to remove a tensor from a computation graph. Note that detach() does not make a copy of the data.\n",
        "\n",
        "2. torch.no_grad() is a context-manager that disabled gradient calculation.\n",
        "Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True.\n",
        "In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True.\n",
        "\n",
        "3. tensor.clone()creates a copy of tensor that imitates the original tensors requires_grad field. We should use clone as a way to copy the tensor while still keeping the copy as a part of the computation graph it came from. Gradients propagating to the cloned tensor will propagate to the original tensor.\n",
        "tensor.clone() maintains the connection with the computation graph. That means, if you use the new cloned tensor, and derive the loss from the new one, the gradients of that loss can be computed all the way back even beyond the point where the new tensor was created.\n",
        "If you want to copy a tensor and detach from the computation graph you should be using **tensor.clone().detach()**\n",
        "\n",
        "4. tensor.backward() computes the gradient of current tensor w.r.t. graph leaves.\n",
        "\n",
        "5. tensor.register_hook() can be very useful to caluclate grad or replace normal grad calulation with customised ones but should be used with caution and only if you're advanced in gradient/derivative calculation."
      ]
    }
  ]
}